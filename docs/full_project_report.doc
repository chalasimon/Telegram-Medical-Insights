Telegram Medical Insights: End-to-End Data Pipeline Report
============================================================

Executive Summary
-----------------
This report documents the development of a robust, modular data pipeline for extracting, transforming, and preparing Telegram data from Ethiopian medical channels for analytics. The pipeline leverages containerization, secure secret management, object-oriented scraping, incremental data collection, and modern ELT practices with dbt, FastAPI, and Dagster orchestration. Engineering decisions focused on automation, maintainability, and scalability, ensuring the solution is production-ready and extensible.

Progress and Completed Works
----------------------------

Task 0 – Project Setup & Environment Management
-----------------------------------------------
- **Git Repository:** Modular structure, clean commit history
- **Python Dependencies:** Managed via `requirements.txt`
- **Dockerization:** `Dockerfile` and `docker-compose.yml` for scraper and PostgreSQL
- **Environment Secrets:** Managed via `.env`, loaded with `python-dotenv`, excluded from Git
- **CI/CD:** GitHub Actions automate scraping and run unit tests in CI

*Rationale:*
- Containerization ensures consistency across environments
- `.env` separates secrets from code
- CI/CD automates testing and reproducibility

Task 1 – Data Scraping and Collection (Extract & Load)
------------------------------------------------------
- **Telegram Scraping:** Telethon-based, object-oriented, incremental
- **Channels:** CheMed, lobelia4cosmetics, tikvahpharma, ethiopianfoodanddrugauthority, etc.
- **Incremental Scraping:** Metadata tracking with `last_scraped.json` prevents duplicates
- **Data Lake Architecture:** Partitioned by date/channel in `data/raw/telegram_messages/YYYY-MM-DD/channel_name.json`; images in `data/images/YYYY-MM-DD/channel_name/`
- **Logging & Monitoring:** Python logging tracks channels, metadata, errors

*Rationale:*
- Incremental logic reduces bandwidth and processing time
- Partitioned structure improves downstream querying
- Robust logging aids debugging and auditing

Task 2 – Data Modeling and Transformation (dbt)
-----------------------------------------------
- **Raw Load:** Python script loads JSON messages into `raw.telegram_messages`
- **dbt Setup:** dbt initialized and connected to PostgreSQL
- **Staging Models:** `stg_telegram_messages.sql` casts types, renames columns, extracts fields
- **Star Schema:** Models for `dim_channels`, `dim_dates`, `fct_messages` created
- **Testing:** dbt tests (not_null, unique, custom business logic) applied

*Rationale:*
- Containerization ensures consistent environments
- Incremental scraping saves resources
- dbt enforces modular SQL transformations and scalable schema design

Completed Tasks
---------------
- Loaded raw JSON data into PostgreSQL
- Initialized and configured dbt project
- Created source for raw data (`telegram_messages`)
- Built staging model (`stg_telegram_messages`)
- Built star schema: `dim_channels`, `dim_dates`, `fct_messages`
- Successfully ran all models with `dbt run`

Data Pipeline Architecture Diagram
----------------------------------
Telegram Channels (CheMed, lobelia4cosmetics, etc.)
    |
    v
Telegram Scraper (Telethon, OO, logging, metadata)
    |
    v
Data Lake (raw JSON, images, partitioned by date/channel)
    |
    v
PostgreSQL (raw, staging, star schema tables)
    |
    v
dbt Models (dim_channels, dim_dates, fct_messages)
    |
    v
FastAPI (analytical API endpoints)
    |
    v
Dagster (orchestration, scheduling, monitoring)

Star Schema Diagram Prompt
--------------------------
                +-------------------+
                |   dim_channels    |
                +-------------------+
                         ^
                         |
                         |
+-------------------+    |    +-------------------+
|   dim_dates       |<---+--->|   fct_messages    |
+-------------------+         +-------------------+
                                   ^
                                   |
                                   |
                          +-------------------+
                          |   dim_products    |  (if applicable)
                          +-------------------+

- fct_messages: Fact table with message-level data (text, media, channel_id, date_id, etc.)
- dim_channels: Channel metadata (name, type, etc.)
- dim_dates: Date dimension (date_id, year, month, day, etc.)
- dim_products: Product dimension (if built, for product mentions or analysis)

Dagster Pipeline Code Example
-----------------------------
from dagster import op, job
from dagster import ScheduleDefinition

@op
def scrape_telegram_data():
    from src.data_scrapper import TelegramScraper
    import asyncio
    channels = [
        "@lobelia4cosmetics",
        "@tikvahpharma",
        "@yetenaweg",
        "@ethiopianfoodanddrugauthority",
        "@CheMed123",
        "@newoptics",
    ]
    scraper = TelegramScraper(env_path="../.env", log_dir="../logs", data_dir="../data", test_mode=False)
    asyncio.run(scraper.scrape_channels(channels, msg_limit=1000))
    return "Scraping completed"

@op
def load_raw_to_postgres():
    import subprocess
    result = subprocess.run([
        "papermill",
        "notebooks/02_data_modelling.ipynb",
        "notebooks/02_data_modelling_output.ipynb"
    ], capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"Notebook execution failed: {result.stderr}")
    return "Raw data loaded to Postgres via notebook"

@op
def run_dbt_transformations():
    import subprocess
    result = subprocess.run(["dbt", "run"], capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"dbt run failed: {result.stderr}")
    return "dbt transformations completed"

@op
def run_yolo_enrichment():
    import subprocess
    result = subprocess.run([
        "papermill",
        "notebooks/03_data_enrichment.ipynb",
        "notebooks/03_data_enrichment_output.ipynb"
    ], capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"YOLO enrichment notebook failed: {result.stderr}")
    return "YOLO enrichment completed via notebook"

@job
def telegram_medical_pipeline():
    scrape_telegram_data()
    load_raw_to_postgres()
    run_dbt_transformations()
    run_yolo_enrichment()

daily_schedule = ScheduleDefinition(
    job=telegram_medical_pipeline,
    cron_schedule="0 0 * * *",  # every day at midnight
)

Challenges & Solutions
----------------------
1. **Incremental Scraping:** Avoided duplicates with `last_scraped.json` metadata tracking.
2. **Raw Data Inconsistencies:** Normalized message fields with a robust parser.
3. **PostgreSQL Ingestion:** Flattened/validated JSON schema for compatibility with dbt.
4. **dbt Modeling/Testing:** Strict naming, foreign key consistency, custom tests.
5. **CI/CD Failures:** Isolated environment-dependent logic; CI only runs dbt models/tests.

Next Steps
----------
1. **Task 3:** Integrate YOLOv8 object detection for image enrichment.
2. **Task 4:** Expose cleaned data and insights via FastAPI backend.
3. **Task 5:** Orchestrate the pipeline with Dagster for end-to-end scheduling and monitoring.

Notes
-----
- The pipeline is modular, robust, and fully orchestrated.
- Each step is automated and can be scheduled via Dagster.
- Data flows from raw extraction to analytics-ready warehouse and API.
- Notebooks are used for ETL and enrichment, integrated via papermill.
- dbt ensures tested, documented, and scalable transformations.
