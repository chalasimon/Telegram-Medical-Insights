Telegram Medical Insights Data Pipeline Documentation
=====================================================

1. Data Pipeline Architecture Diagram
-------------------------------------

Telegram Channels (CheMed, lobelia4cosmetics, etc.)
    |
    v
Telegram Scraper (Telethon, OO, logging, metadata)
    |
    v
Data Lake (raw JSON, images, partitioned by date/channel)
    |
    v
PostgreSQL (raw, staging, star schema tables)
    |
    v
dbt Models (dim_channels, dim_dates, fct_messages)
    |
    v
FastAPI (analytical API endpoints)
    |
    v
Dagster (orchestration, scheduling, monitoring)

2. Star Schema Diagram Prompt
-----------------------------

                +-------------------+
                |   dim_channels    |
                +-------------------+
                         ^
                         |
                         |
+-------------------+    |    +-------------------+
|   dim_dates       |<---+--->|   fct_messages    |
+-------------------+         +-------------------+
                                   ^
                                   |
                                   |
                          +-------------------+
                          |   dim_products    |  (if applicable)
                          +-------------------+

- fct_messages: Fact table with message-level data (text, media, channel_id, date_id, etc.)
- dim_channels: Channel metadata (name, type, etc.)
- dim_dates: Date dimension (date_id, year, month, day, etc.)
- dim_products: Product dimension (if built, for product mentions or analysis)

3. Dagster Pipeline Code Example
--------------------------------

from dagster import op, job
from dagster import ScheduleDefinition

@op
def scrape_telegram_data():
    from src.data_scrapper import TelegramScraper
    import asyncio
    channels = [
        "@lobelia4cosmetics",
        "@tikvahpharma",
        "@yetenaweg",
        "@ethiopianfoodanddrugauthority",
        "@CheMed123",
        "@newoptics",
    ]
    scraper = TelegramScraper(env_path="../.env", log_dir="../logs", data_dir="../data", test_mode=False)
    asyncio.run(scraper.scrape_channels(channels, msg_limit=1000))
    return "Scraping completed"

@op
def load_raw_to_postgres():
    import subprocess
    result = subprocess.run([
        "papermill",
        "notebooks/02_data_modelling.ipynb",
        "notebooks/02_data_modelling_output.ipynb"
    ], capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"Notebook execution failed: {result.stderr}")
    return "Raw data loaded to Postgres via notebook"

@op
def run_dbt_transformations():
    import subprocess
    result = subprocess.run(["dbt", "run"], capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"dbt run failed: {result.stderr}")
    return "dbt transformations completed"

@op
def run_yolo_enrichment():
    import subprocess
    result = subprocess.run([
        "papermill",
        "notebooks/03_data_enrichment.ipynb",
        "notebooks/03_data_enrichment_output.ipynb"
    ], capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"YOLO enrichment notebook failed: {result.stderr}")
    return "YOLO enrichment completed via notebook"

@job
def telegram_medical_pipeline():
    scrape_telegram_data()
    load_raw_to_postgres()
    run_dbt_transformations()
    run_yolo_enrichment()

daily_schedule = ScheduleDefinition(
    job=telegram_medical_pipeline,
    cron_schedule="0 0 * * *",  # every day at midnight
)

4. Notes
--------
- The pipeline is modular, robust, and fully orchestrated.
- Each step is automated and can be scheduled via Dagster.
- Data flows from raw extraction to analytics-ready warehouse and API.
- Notebooks are used for ETL and enrichment, integrated via papermill.
- dbt ensures tested, documented, and scalable transformations.
